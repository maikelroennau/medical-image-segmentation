{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Dropout, concatenate, Conv2DTranspose\n",
    "\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 256\n",
    "steps_per_epoch = 256\n",
    "\n",
    "image_batch_size = 256\n",
    "augmentation_batch_size = 16\n",
    "\n",
    "height = 240 # 240 480  960 1920\n",
    "width = 320 # 320 640 1280 2560\n",
    "input_shape = (height, width, 3)\n",
    "\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size=16, target_size=(1920, 2560), augmented_dir=\"dataset/augmentation/\", seed=2149):\n",
    "    datagen_arguments = dict(\n",
    "        # featurewise_center=True,\n",
    "        # featurewise_std_normalization=True,\n",
    "        rotation_range=0.2,\n",
    "        width_shift_range=0.05,\n",
    "        height_shift_range=0.05,\n",
    "        shear_range=0.05,\n",
    "        zoom_range=0.05,\n",
    "        fill_mode=\"reflect\",\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        rescale=1./255.\n",
    "    )\n",
    "\n",
    "    image_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**datagen_arguments)\n",
    "    mask_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**datagen_arguments)\n",
    "\n",
    "    images = image_datagen.flow_from_directory(\n",
    "        directory=\"dataset/train/\",\n",
    "        target_size=target_size,\n",
    "        classes=[\"images\"],\n",
    "        class_mode=None,\n",
    "        color_mode=\"rgb\",\n",
    "        batch_size=batch_size,\n",
    "        # save_to_dir=f\"{augmented_dir}/images\",\n",
    "        save_prefix=\"image\",\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    masks = mask_datagen.flow_from_directory(\n",
    "        directory=\"dataset/train/\", \n",
    "        target_size=target_size,\n",
    "        classes=[\"masks\"],\n",
    "        class_mode=None,\n",
    "        color_mode=\"rgb\",\n",
    "        batch_size=batch_size,\n",
    "        # save_to_dir=f\"{augmented_dir}/masks\",\n",
    "        save_prefix=\"mask\",\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    for images, masks in zip(images, masks):\n",
    "        yield (images, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = data_generator(augmentation_batch_size, (height, width))\n",
    "\n",
    "# for i, batch in enumerate(generator):\n",
    "#     print(batch[0].shape, batch[1].shape)\n",
    "#     if i >= image_batch_size - 1:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-tobacco",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_path = \"dataset/augmentation/images/\"\n",
    "# masks_path = \"dataset/augmentation/masks/\"\n",
    "\n",
    "# images = os.listdir(images_path)\n",
    "# masks = os.listdir(masks_path)\n",
    "# images.sort()\n",
    "# masks.sort()\n",
    "\n",
    "# limit = len(images)\n",
    "\n",
    "# print(f\"Total images: {limit}\")\n",
    "# print(f\"Target shape: {(limit, height, width, 3)}\")\n",
    "\n",
    "# images_tensor = np.empty((limit, height, width, 3))\n",
    "# masks_tensor = np.empty((limit, height, width, 1))\n",
    "\n",
    "# for i, (image, mask) in enumerate(zip(images, masks)):\n",
    "#     assert image.split(\"_\")[-1] == mask.split(\"_\")[-1], f\"Image and maks do not correspond: {image}, {mask}\"\n",
    "#     img = cv2.imread(os.path.join(images_path, image))\n",
    "#     images_tensor[i, :, :, :] = img\n",
    "\n",
    "#     msk = cv2.imread(os.path.join(masks_path, mask), cv2.IMREAD_GRAYSCALE)\n",
    "#     masks_tensor[i, :, :, 0] = msk\n",
    "#     # if i + 1 == limit:\n",
    "#         # break\n",
    "\n",
    "# print(images_tensor.shape)\n",
    "# print(masks_tensor.shape)\n",
    "\n",
    "# def yield_data(x, y):\n",
    "#     for xx, yy in zip(x, y):\n",
    "#         yield (xx, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_distance_loss(y_true, y_pred, smooth=100):\n",
    "    \"\"\"\n",
    "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
    "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
    "    \n",
    "    The jaccard distance loss is usefull for unbalanced datasets. This has been\n",
    "    shifted so it converges on 0 and is smoothed to avoid exploding or disapearing\n",
    "    gradient.\n",
    "    \n",
    "    Ref: https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    \n",
    "    @url: https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
    "    @author: wassname\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth\n",
    "\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1.):\n",
    "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
    "    return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    \n",
    "    conv1 = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(pool3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(pool4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(conv5)\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding=\"same\")(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding=\"same\")(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding=\"same\")(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(conv8)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding=\"same\")(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(conv9)\n",
    "\n",
    "    conv10 = Conv2D(1, (1, 1), activation=\"sigmoid\")(conv9)\n",
    "\n",
    "    model = keras.Model(inputs=[inputs], outputs=[conv10])\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=learning_rate), loss=dice_coef_loss, metrics=[dice_coef, \"binary_accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = make_model(input_shape=input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_directory = os.path.join(\"checkpoints\", f\"{time.strftime('%Y%m%d%H%M%S')}\")\n",
    "os.makedirs(checkpoint_directory)\n",
    "\n",
    "with open(os.path.join(checkpoint_directory, \"hyperparameters.txt\"), \"w\") as hyperparameters:\n",
    "    hyperparameters.write(f\"epochs: {epochs}\\n\")\n",
    "    hyperparameters.write(f\"batch_size: {batch_size}\\n\")\n",
    "    hyperparameters.write(f\"steps_per_epoch: {steps_per_epoch}\\n\")\n",
    "    hyperparameters.write(f\"image_batch_size: {image_batch_size}\\n\")\n",
    "    hyperparameters.write(f\"augmentation_batch_size: {augmentation_batch_size}\\n\")\n",
    "    hyperparameters.write(f\"height: {height}\\n\")\n",
    "    hyperparameters.write(f\"width: {width}\\n\")\n",
    "    hyperparameters.write(f\"input_shape: {input_shape}\\n\")\n",
    "    hyperparameters.write(f\"learning_rate: {learning_rate}\\n\")\n",
    "\n",
    "callbacks = [\n",
    "    #keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1,  mode=\"auto\", cooldown=1),\n",
    "    keras.callbacks.ModelCheckpoint(os.path.join(checkpoint_directory, \"epoch_{epoch}.h5\"), monitor=\"val_loss\", save_best_only=False),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print(f\"Training start - {time.strftime('%x %X')}\")\n",
    "print(f\"  - Epochs: {epochs}\")\n",
    "print(f\"  - Batch size: {batch_size}\")\n",
    "print(f\"  - Learning rate: {model.optimizer.get_config()['learning_rate']}\\n\")\n",
    "\n",
    "keras.backend.clear_session()\n",
    "history = model.fit(generator, batch_size=batch_size, epochs=epochs, steps_per_epoch=steps_per_epoch, callbacks=callbacks)#, validation_data=val_ds)\n",
    "# history = model.fit(images_tensor, masks_tensor, batch_size=8, epochs=5, steps_per_epoch=8, callbacks=callbacks)#, validation_data=val_ds)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"\\nTraining end - {time.strftime('%x %X')}\")\n",
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"Duration: {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "print(f\"  - Learning rate: {model.optimizer.get_config()['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_path = \"dataset/test/\"\n",
    "\n",
    "images = os.listdir(test_images_path)\n",
    "\n",
    "test_images_tensor = np.empty((len(images), height, width, 3))\n",
    "original_shape = None\n",
    "\n",
    "for i, image_path in enumerate(images):\n",
    "    image = cv2.imread(os.path.join(test_images_path, image_path), cv2.IMREAD_COLOR)\n",
    "    original_shape = image.shape[:2][::-1]\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (width, height))\n",
    "    test_images_tensor[i, :, :, :] = image\n",
    "    \n",
    "print(test_images_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all = False\n",
    "\n",
    "if test_all:\n",
    "    for i in range(1, epochs):\n",
    "        loaded_model = keras.models.load_model(f\"{checkpoint_directory}/epoch_{i}.h5\", custom_objects={\"dice_coef_loss\": dice_coef_loss, \"dice_coef\": dice_coef})\n",
    "        predictions = loaded_model.predict(test_images_tensor, verbose=1)\n",
    "        \n",
    "        figures = {}\n",
    "\n",
    "        for j, prediction in enumerate(predictions):\n",
    "            cv2.imwrite(os.path.join(test_images_path, f\"{i}_{j}.jpg\"), prediction * 255)\n",
    "            figures[f\"{i}_{j}.jpg\"] = prediction\n",
    "\n",
    "        print(np.unique(predictions))\n",
    "else:\n",
    "    loaded_model = keras.models.load_model(f\"{checkpoint_directory}/epoch_{epocs}.h5\", custom_objects={\"dice_coef_loss\": dice_coef_loss, \"dice_coef\": dice_coef})\n",
    "    predictions = loaded_model.predict(test_images_tensor, verbose=1)\n",
    "\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        name = os.path.basename(images[i]).split(\".\")[0]\n",
    "        prediction = cv2.resize(prediction, original_shape)\n",
    "        cv2.imwrite(os.path.join(test_images_path, f\"{name}_prediction.jpg\"), prediction * 255)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
